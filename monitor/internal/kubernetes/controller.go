package kubernetesmonitor

import (
	"context"

	"go.aporeto.io/trireme-lib/common"
	"go.aporeto.io/trireme-lib/monitor/config"
	"go.aporeto.io/trireme-lib/monitor/extractors"
	"go.aporeto.io/trireme-lib/policy"
	"go.uber.org/zap"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/runtime"

	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/manager"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
	"sigs.k8s.io/controller-runtime/pkg/source"
)

// newReconciler returns a new reconcile.Reconciler
func newReconciler(mgr manager.Manager, handler *config.ProcessorConfig, metadataExtractor extractors.KubernetesMetadataExtractorType, nodeName string, enableHostPods bool) *ReconcilePod {
	return &ReconcilePod{
		client:            mgr.GetClient(),
		scheme:            mgr.GetScheme(),
		handler:           handler,
		metadataExtractor: metadataExtractor,
		nodeName:          nodeName,
		enableHostPods:    enableHostPods,
	}
}

// addController adds a new Controller to mgr with r as the reconcile.Reconciler
func addController(mgr manager.Manager, r *ReconcilePod, eventsCh <-chan event.GenericEvent) error {
	// Create a new controller
	c, err := controller.New("pod-controller", mgr, controller.Options{Reconciler: r})
	if err != nil {
		return err
	}

	// use the our watch pod mapper which filters pods before we reconcile
	if err := c.Watch(&source.Kind{Type: &corev1.Pod{}}, &handler.EnqueueRequestsFromMapFunc{
		ToRequests: &WatchPodMapper{
			client:         mgr.GetClient(),
			nodeName:       r.nodeName,
			enableHostPods: r.enableHostPods,
		}}); err != nil {
		return err
	}

	// we pass in a custom channel for events generated by resync
	if err := c.Watch(&source.Channel{Source: eventsCh}, &handler.EnqueueRequestForObject{}); err != nil {
		return err
	}

	return nil
}

var _ reconcile.Reconciler = &ReconcilePod{}

// ReconcilePod reconciles a Pod object
type ReconcilePod struct {
	// This client, initialized using mgr.Client() above, is a split client
	// that reads objects from the cache and writes to the apiserver
	client            client.Client
	scheme            *runtime.Scheme
	handler           *config.ProcessorConfig
	metadataExtractor extractors.KubernetesMetadataExtractorType
	nodeName          string
	enableHostPods    bool
}

// Reconcile reads that state of the cluster for a pod object
func (r *ReconcilePod) Reconcile(request reconcile.Request) (reconcile.Result, error) {
	ctx := context.TODO()
	puID := request.NamespacedName.String()

	// Fetch the corresponding pod object.
	pod := &corev1.Pod{}
	if err := r.client.Get(ctx, request.NamespacedName, pod); err != nil {
		if errors.IsNotFound(err) {
			zap.L().Info("pod not found", zap.String("puID", puID))
			if err := r.handler.Policy.HandlePUEvent(
				ctx,
				puID,
				common.EventDestroy,
				policy.NewPURuntimeWithDefaults(),
			); err != nil {
				zap.L().Error("failed to handle destroy event", zap.String("puID", puID), zap.Error(err))
			}
			return reconcile.Result{}, nil
		}
		// Otherwise, we retry.
		return reconcile.Result{}, err
	}

	if pod.Spec.HostNetwork {
		return reconcile.Result{}, nil
	}

	runtime, err := r.metadataExtractor(pod)
	if err != nil {
		zap.L().Error("failed to extract metadata", zap.String("puID", puID), zap.Error(err))
		return reconcile.Result{}, err
	}

	switch pod.Status.Phase {
	case corev1.PodPending:
		zap.L().Info("pod pending", zap.String("puID", puID))
		if err := r.handler.Policy.HandlePUEvent(
			ctx,
			puID,
			common.EventCreate,
			runtime,
		); err != nil {
			zap.L().Error("failed to handle create event", zap.String("puID", puID), zap.Error(err))
		}
	case corev1.PodRunning:
		zap.L().Info("pod running", zap.String("puID", puID))
		if err := r.handler.Policy.HandlePUEvent(
			ctx,
			puID,
			common.EventStart,
			runtime,
		); err != nil {
			zap.L().Error("failed to handle start event", zap.String("puID", puID), zap.Error(err))
		}
	case corev1.PodSucceeded:
		zap.L().Info("pod succeeded", zap.String("puID", puID))
		if err := r.handler.Policy.HandlePUEvent(
			ctx,
			puID,
			common.EventStop,
			runtime,
		); err != nil {
			zap.L().Error("failed to handle stop event", zap.String("puID", puID), zap.Error(err))
		}
	case corev1.PodFailed:
		zap.L().Info("pod failed", zap.String("puID", puID))
		if err := r.handler.Policy.HandlePUEvent(
			ctx,
			puID,
			common.EventStop,
			runtime,
		); err != nil {
			zap.L().Error("failed to handle stop event", zap.String("puID", puID), zap.Error(err))
		}
	case corev1.PodUnknown:
		zap.L().Error("pod is in unknown state", zap.String("puID", puID), zap.Error(err))
	default:
		zap.L().Info("unknown pod phase", zap.String("podPhase", string(pod.Status.Phase)))
	}

	return reconcile.Result{}, nil
}
